---
title: "Methods for Dimensionality Reduction"
author: "Ramón Roales-Welsch"
date: "11 Januar 2018"
output:
  pdf_document:
    toc: true
    toc_depth: 2
---

```{r setup, cache=FALSE, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, cache=TRUE)


library(knitr)
library(readr)
library(dplyr)
library(tidytext)
library(tokenizers)
library(ggplot2)
library(tidyr)
library(scales)
library(stringr)
library(grid)
library(gridExtra)
```

```{r data_sample}
source('data_sample.R')

```

\newpage
# Introduction
## Overview

1. Multidimensional Scaling (MDS)
    - Dimension reduction by using feature-distances between observations

2. Principal Components Analysis (PCA)
    - Reduce dimensions while maximizing variance of features
    - Slide 88: PCA is not applicable in Big Data (*FastMap* method is proposed.)

3. Feature Selection
    - Drop weak features and select features with strong explanatory power


## Motivation, Goal and Selection Criteria

Wikipedia names three main motivations of dimensionality reduction: Reduce costs (time and storage), reduce multicollinearity and provide visualisation oportunities. In the following the goals and selection criteria for mentioned methods are discussed:

1. MDS
    - Goal: Visualize your dataset in a 2-dimensional space and to show the similarities between objects(observations).
    - Alternative to Factor Analysis: Focus on dissimilarities (distances) between objects rather than similarities between features (via correlation matrices).

2. PCA
    - Goal: Dimension reduction and eliminitaion of correlation between features. (*Bishop 2006*: Dimensionality reduction, lossy data compression, feature extraction and data visualitation.)
    - Projects $d$ features on $m$ features with $m < d$.

3. Feature Selection
    - Goal: Reduce overfitting, improve generalization and speed up computation.
    - Either drop weak features or select strong features.


\newpage

# Multidimensional Scaling (MDS)

## Classical MDS

To build up the 2-dimensional matrix $D = [d_{i,j}]$, you need to compute the single components of the features $x$ and $y$. Therefore, you use the subsequent formula according to Wikipedia:


$$d_{i,j} = \sqrt{(x_i - x_j)^2 + (y_i - y_j)^2}$$

Lecture Slide 76 gives the formula for $d-dimensions$:

$$d(x_i, x_j)^2 = \sum_{k = 1}^{d}{(x_{i,k} - x_{j,k})^2}$$
**Not found on slides** In order to evaluate, whether the MDS provides a useful dimensionality reduction, one need to examine the Stress/Strain (refering to the information loss) of the reduction.

## Metric MDS

$$Stress_D(x_1, x_2, ..., x_N) = \Bigg(\frac{\sum_{i,j}(d_{i,j} - \begin{Vmatrix}x_i - x_j \end{Vmatrix})^2}{\sum_{i,j} d_{i,j}^2}\Bigg)^{1/2}$$

## Non-metric MDS

$$Stress = \sqrt{\frac{\sum{(f(x) - d)^2}}{\sum{d^2}}}$$
\newpage

# Principal Components Analysis (PCA) - Slides 81-88


## Projection to $m$-Dimensions
PCA projects $x \in \mathbb{R}^d$ to $z \in \mathbb{R}^m$. When $\begin{Vmatrix}w \end{Vmatrix} = 1$ holds, the following transformation can be applied:

$$z = w^T x$$

## Maximizing Variance

When we maximize the variance of our projection $z$ and therefore minimize the information loss of the transformation, $\begin{Vmatrix}w_1 \end{Vmatrix} = w_1^T * w_1 =  1$ must hold. Otherwise $\begin{Vmatrix}w_1 \end{Vmatrix} \rightarrow \infty$ applies and the variables get inflated to $\infty$. When we project $x_i$ with $z_i = w_1^T x_i$, the mean $\bar{x} = \frac{1}{N}\sum_{i=1}^{N}x_i$ of the projection then is $\bar{z} = w_1^T\bar{x}$. From this follows the variance definition, which we want to maximize (Slide 83):

$$\frac{1}{N} \sum_{i=1}^{N} (w_1^T x_i - w_1^T \bar{x})^2 = \frac{1}{N} \sum_{i=1}^{N} \big(w_1^T( x_i - \bar{x})\big)^2 =w_1^T \bigg[ \frac{1}{N} \sum_{i=1}^{N} (x_i - \bar{x})*(x_i - \bar{x})^T \bigg] w_1 = w_1^T \Sigma w_1$$

$\Sigma$ is the $m$ x $m$ covariance matrix and further referred to as $\Sigma^{(m)}$ to indicate the $m$-dimensions. Using the langrange multiplier $\lambda_1 \in \mathbb{R}_{\leq0}$, we have the following optimization problem (Slide 84):

$$\underset{w_1}{\text{maximize}} ~~ f(w_1) = w_1^T \Sigma w_1 + \lambda_1 (1 - w_1^T w_1)$$

$$f'(w_1) \rightarrow \Sigma w_1 - \lambda_1w_1 \mathop{=}\limits^! 0$$
Following Slide 85, two observations can be made:

1. With $\Sigma w_1 = \lambda_1 w_1$ or $w_1^T \Sigma w_1 = \lambda_1$, $w_1$ have to be an 'eigenvector' of $\Sigma$, which is scaling the projection $\Sigma w_1$.

2. The eigenvector with the highest eigenvalue of $w_1$ maximizes the variance of the projection.

## Eliminate Correlation between Features

We seek an orthognal relation between projected feature $z_1$ and $z_j$, which means the correlation between the features is equal to zero: $cor(z_1, z_2) = 0$. This lead to the condition $w_2^T w_1 = 0$ and the following optimization problem:

$$\underset{w_2}{\text{maximize}} ~~ f(w_2) = w_1^T \Sigma w_1 + \lambda_2 (1 - w_1^T w_1) + \beta (0 - w_2^T w_1)$$
<Slide 87> The first-order derivation is set to zero: $f'(w_2) \rightarrow 2 \Sigma w_2 - 2 \lambda_2 w_2 - \beta w_1 \mathop{=}\limits^! 0$ When we multiplicate this term from the left with $w_1$ we obtain the following expression:

$$2 w_1^T \Sigma w_2 - 2 \lambda_2 w_1^Tw_2 - \beta w_1^Tw_1 \mathop{=}\limits^! 0$$
As $w_1^Tw_1 = 1$, then $\beta = 0$. We conditioned $w_1^Tw_2 = 0$ and observe $w_1^T \Sigma w_2 = w_2^T \Sigma w_1 = w_1^T \lambda_1 w_2 = 0$.

Finally, we have a result, which is analogous to $w_1$: 

$$\Sigma w_2 = \lambda_2 w_2$$

\newpage

#Feature Selection

## Overview (Slide 89/90)

1. Evaluate the 'quality' of a feature.
2. Find best subset of features using the quality criterium.
    * highest quality of featires
    * Lowest multicollinearity in subset
    
Use greddy algorithm with either bottom-up or top-down approach:

- bottom-up: Start with no feature and add new feature with every iteration. Evaluation via an error function and repeated until the required dimension is achieved.

- top-down: Start with all feature and remove the least explanatory with each step under consideration of an error function and until the required dimension is achieved.

## Outlier and Duplicates

### Outlier (Slide 101/102)

* A boxplot can reveal outlier. 
* Also mean and standard deviation (sd) can be used. Usually outlier can be identified by $mean \pm sd$.
* Use cluster, every observation, which does not belong to a cluster, is a outlier.
* Parametrized processes, for instance mixture of gaussians.

### Duplicates (Slide 103)

Use distances to find duplicates or observations which are very similar (very small distance). Reduce the number of observations and only keep one of them.


\newpage
# Appendix: Technical Implementation
## Data Sample

```
# Code for Data Sample

Mirror the code here. It is executed in a separate document.

```


## R


## Python (sklearn - package)

### Eigenvalue and Eigenvector (Numpy)

You can calculate 'eigenvalue' and 'eigenvector' with the function *numpy.linalg.eig()*.

### Useful function/package

sklearn.manifold.MDS(n_components=2, metric=True, n_init=4, max_iter=300, verbose=0, eps=0.001, n_jobs=1, random_state=None, dissimilarity=’euclidean’)

http://scikit-learn.org/stable/modules/generated/sklearn.manifold.MDS.html

\newpage
# Appendix: Code examples

## Example: Table

```{r example_table, results='asis'}
kable(summary(cars), 
      caption = "Example Summary Cars Dataset")

```

## Example: Plot

```{r example_plot, figure=TRUE, fig.align = "center", fig.width = 4, fig.asp =  1}
plot(cars, 
      main = "Example Cars Dataset")

```


\newpage
# Appendix: Literature Sources

* Lecture Slides from Machine Learning, Dr. Thomas Fober
* Bishop, C. (2007). *Pattern Recognition and Machine Learning* (Information Science and Statistics), 1st edn. 2006. corr. 2nd printing edn. Springer, New York.
* Wikipedia
    - https://en.wikipedia.org/wiki/Multidimensional_scaling
    - https://en.wikipedia.org/wiki/Principal_component_analysis
    - https://en.wikipedia.org/wiki/Feature_selection
* STUFF
    - STUFF
    

# Appendix: Covariation Matrix Calculation (More formulas)

$$\Sigma^{(m)} = E\bigg[ (X^{(m)} - E[X^{(m)}])(X^{(m)} - E[X^{(m)}])^T\bigg]$$


$$\Sigma^{(m)} = E\bigg[ \big(X^{(m)} - \frac{1}{N} \sum_{i=1}^{N}X_i^{(m)}\big)\big(X^{(m)} - \frac{1}{N} \sum_{i=1}^{N}X_i^{(m)}\big)^T\bigg]$$


$$\Sigma^{(m)} = \sum_{i=1}^{N} \frac{1}{N}\bigg[ (x_i^{(m)} - \bar{x}^{(m)})(x_i^{(m)} - \bar{x}^{(m)})^T\bigg]$$
